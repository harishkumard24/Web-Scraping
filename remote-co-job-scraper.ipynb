{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dabf351-1913-4c1d-8159-6619e3a07205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 links from a page. Total so far: 50\n",
      "Found 50 links from a page. Total so far: 100\n",
      "Found 50 links from a page. Total so far: 150\n",
      "Found 50 links from a page. Total so far: 200\n",
      "Found 50 links from a page. Total so far: 250\n",
      "Found 50 links from a page. Total so far: 300\n",
      "Found 50 links from a page. Total so far: 350\n",
      "Found 50 links from a page. Total so far: 400\n",
      "Found 50 links from a page. Total so far: 450\n",
      "Found 50 links from a page. Total so far: 500\n",
      "Found 50 links from a page. Total so far: 550\n",
      "Found 50 links from a page. Total so far: 600\n",
      "Found 50 links from a page. Total so far: 650\n",
      "Found 50 links from a page. Total so far: 700\n",
      "Found 50 links from a page. Total so far: 750\n",
      "Found 50 links from a page. Total so far: 800\n",
      "Found 50 links from a page. Total so far: 850\n",
      "Found 50 links from a page. Total so far: 900\n",
      "Found 30 links from a page. Total so far: 930\n",
      "Found 50 links from a page. Total so far: 980\n",
      "Found 50 links from a page. Total so far: 1030\n",
      "Found 50 links from a page. Total so far: 1080\n",
      "Found 50 links from a page. Total so far: 1130\n",
      "Found 50 links from a page. Total so far: 1180\n",
      "Found 50 links from a page. Total so far: 1230\n",
      "Found 50 links from a page. Total so far: 1280\n",
      "\n",
      "Step 1 complete. Total job links found: 1280\n",
      "\n",
      "Scraped 50/1280 jobs\n",
      "Scraped 100/1280 jobs\n",
      "Scraped 150/1280 jobs\n",
      "Scraped 200/1280 jobs\n",
      "Scraped 250/1280 jobs\n",
      "Scraped 300/1280 jobs\n",
      "Scraped 350/1280 jobs\n",
      "Scraped 400/1280 jobs\n",
      "Scraped 450/1280 jobs\n",
      "Scraped 500/1280 jobs\n",
      "Scraped 550/1280 jobs\n",
      "Scraped 600/1280 jobs\n",
      "Scraped 650/1280 jobs\n",
      "Scraped 700/1280 jobs\n",
      "Scraped 750/1280 jobs\n",
      "Scraped 800/1280 jobs\n",
      "Scraped 850/1280 jobs\n",
      "Scraped 900/1280 jobs\n",
      "Scraped 950/1280 jobs\n",
      "Scraped 1000/1280 jobs\n",
      "Scraped 1050/1280 jobs\n",
      "Scraped 1100/1280 jobs\n",
      "Scraped 1150/1280 jobs\n",
      "Scraped 1200/1280 jobs\n",
      "Scraped 1250/1280 jobs\n",
      "\n",
      "Step 2 complete. Total jobs scraped: 1263\n",
      "\n",
      "[+] Saved 1263 jobs to job_data1.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "BASE_URL = \"https://remote.co/remote-jobs/developer\"\n",
    "user_agent = UserAgent()\n",
    "\n",
    "all_job_links = []\n",
    "\n",
    "# ---------------- Step 1: Fetch all job links with retries ----------------\n",
    "def fetch_links(url, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers={\"User-Agent\": user_agent.random}, timeout=15)\n",
    "            if resp.status_code == 200:\n",
    "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "                return [\"https://remote.co\" + a[\"href\"] for a in soup.select(\"a[href*='/job-details/']\")]\n",
    "        except:\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "    return []\n",
    "\n",
    "page_urls = [f\"{BASE_URL}?page={i}\" if i > 1 else BASE_URL for i in range(1, 28)]\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(fetch_links, url) for url in page_urls]\n",
    "    for future in as_completed(futures):\n",
    "        links = future.result()\n",
    "        if links:\n",
    "            all_job_links.extend(links)\n",
    "            print(f\"Found {len(links)} links from a page. Total so far: {len(all_job_links)}\")\n",
    "\n",
    "print(f\"\\nStep 1 complete. Total job links found: {len(all_job_links)}\\n\")\n",
    "\n",
    "# ---------------- Step 2: Scrape job details with retries ----------------\n",
    "all_jobs = []\n",
    "\n",
    "def fetch_job_data(job_url, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(job_url, headers={\"User-Agent\": user_agent.random}, timeout=15)\n",
    "            if resp.status_code == 200:\n",
    "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "                \n",
    "                data = {\n",
    "                    \"Title\": soup.select_one(\"h1\").get_text(strip=True) if soup.select_one(\"h1\") else \"\",\n",
    "                    \"Company\": soup.select_one(\"h3\").get_text(strip=True) if soup.select_one(\"h3\") else \"\",\n",
    "                    \"Remote Work Level\": \"\",\n",
    "                    \"Location\": \"\",\n",
    "                    \"Salary\": \"\",\n",
    "                    \"Job Type\": \"\",\n",
    "                    \"Job Schedule\": \"\",\n",
    "                    \"Career Level\": \"\",\n",
    "                    \"Education Level\": \"\",\n",
    "                    \"Categories\": \"\",\n",
    "                    \"URL\": job_url\n",
    "                }\n",
    "\n",
    "                for li in soup.select(\"ul#detail-list-wrapper li\"):\n",
    "                    labels = li.find_all(\"p\")\n",
    "                    if len(labels) >= 2:\n",
    "                        key = labels[0].get_text(strip=True).rstrip(\":\")\n",
    "                        val = labels[1].get_text(strip=True)\n",
    "                        if key in data:\n",
    "                            data[key] = val\n",
    "\n",
    "                return data\n",
    "        except:\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    futures = [executor.submit(fetch_job_data, url) for url in all_job_links]\n",
    "    for i, future in enumerate(as_completed(futures), 1):\n",
    "        job = future.result()\n",
    "        if job:\n",
    "            all_jobs.append(job)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Scraped {i}/{len(all_job_links)} jobs\")\n",
    "\n",
    "print(f\"\\nStep 2 complete. Total jobs scraped: {len(all_jobs)}\\n\")\n",
    "\n",
    "# ---------------- Step 3: Save to CSV ----------------\n",
    "if all_jobs:\n",
    "    with open(\"job_data1.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        keys = all_jobs[0].keys()\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_jobs)\n",
    "    print(f\"[+] Saved {len(all_jobs)} jobs to job_data1.csv\")\n",
    "else:\n",
    "    print(\"No job data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115363da-9588-4080-baa4-ef72ce96c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593cb23-150e-4e2a-ba2e-b1d79894ea65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
